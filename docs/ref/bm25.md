# Combining Embeddings with BM25 for Chunk Indexing

Combining Embeddings with BM25 for Chunk Indexing: Concepts, Implementations, and Hybrid Strategies

## Introduction

Retrieval-Augmented Generation (RAG) systems, designed to augment large language models (LLMs) by injecting relevant external knowledge, stand or fall on their information retrieval component. Recent advances have seen the rise of hybrid retrieval strategies that combine dense vector embeddings and classical lexical search, especially with BM25. Anthropic’s "Contextual Retrieval" methodology highlights how mixing dense and sparse retrieval over intelligently chunked, contextualized text can dramatically improve retrieval success rates—reducing failures by up to 67% when paired with reranking models. This report presents a thorough, research-driven exploration of BM25’s theoretical and practical foundation, its integration in LLM-oriented pipelines, hybrid strategies for marrying embeddings and keyword scoring, best practices around chunking and indexing, and actionable implementation examples in Go, including a minimal working hybrid retriever.

## I. BM25: Mathematical Foundations and Contemporary Relevance

### 1. Origins and Motivation

BM25 (Best Matching 25) is a probabilistic ranking function rooted in the probabilistic information retrieval framework developed over several decades by Robertson, Spärck Jones, and colleagues. Born as the default scoring function in the Okapi Information Retrieval System, BM25 improved on TF-IDF by refining the treatment of term rarity, saturation effects in term frequency, and length normalization—in effect blending statistical rigor with practical adaptability.

BM25 has been pivotal in both academic and industrial search, powering anything from small-scale document retrieval to the backbone ranking in engines like Elasticsearch, Apache Lucene, and OpenSearch.

### 2. The BM25 Scoring Formula, Step by Step

The BM25 score for a document D with respect to a query Q composed of terms q₁, ..., qₙ is:

$$
Score(D,Q)=∑i=1nIDF(qi)⋅f(qi,D)⋅(k1+1)f(qi,D)+k1⋅(1−b+b⋅∣D∣avgdl)\mathrm{Score}(D, Q) = \sum_{i=1}^n IDF(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\mathrm{avgdl}})}
$$

- **f(qᵢ, D)**: frequency of term qᵢ in document D.
- **|D|**: length of document D, usually in tokens or words.
- **avgdl**: average document length in the corpus.
- **k₁**: term frequency saturation parameter (typ. 1.2 – 2.0); controls how quickly additional occurrences of a term lose impact.
- **b**: length normalization parameter (typ. 0.75); 0 means no normalization, 1 means full normalization.
- **IDF(qᵢ)**: inverse document frequency, rewarding rare terms:

$$
IDF(qi)=ln⁡(N−n(qi)+0.5n(qi)+0.5+1)IDF(q_i) = \ln \left( \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} + 1 \right)
$$

Where:

- **N**: total number of documents.
- **n(qᵢ)**: number of documents containing term qᵢ.

### Worked Example

Suppose "quick" and "brown" appear in 2 out of 3 docs each, and a query is "quick brown". For a document of length 9 (with one occurrence each), one computes IDF as ≈0.47. The TF component for "quick" is:

$$
TF=1×2.21+1.2×(1−0.75+0.75×(9/7))≈0.895TF = \frac{1 \times 2.2}{1 + 1.2 \times (1-0.75+0.75 \times (9/7))} \approx 0.895
$$

So the contribution for "quick" is 0.895 × 0.47 ≈ 0.42 etc. Sum across all terms for the final relevance.

### 3. Why BM25 Remains Relevant for Modern NLP and LLM Applications

BM25’s enduring utility stems from several core strengths:

- **Interpretable and Robust**: BM25’s scoring depends on transparent statistics, making debugging and explainability easy.
- **Efficient and Scalable**: Inverted indices enable sub-second, large-corpus search.
- **Parameterizable**: k₁ and b allow tuning for dataset/domain specifics.
- **Out-of-the-Box Quality**: Requires no fine-tuning—robust across domains (cf. BEIR/zero-shot benchmarks).

BM25 serves as a strong first-stage retriever in open-domain question answering, legal/medical information search, and is often used alongside or upstream of neural ranking models—sometimes providing high-ranking recall even where dense retrievers struggle.

**However**, BM25 is fundamentally a _lexical_ method. It matches on words, not meaning—missing synonyms, paraphrases, and broader semantic links, where neural embeddings excel.

### 4. BM25’s Probabilistic Model in Practice

At the core lies the “binary independence model”, which estimates a document’s query relevance probability by assuming term independence and binary term presence. While modern BM25 scores are not direct probabilities, their strong empirical track record and parameterization enable effective ranking for practical search engines, LLM context windows, and hybrid retrieval applications.

## II. Hybrid Retrieval: Why Combine BM25 and Embeddings?

### 1. Motivation and Empirical Evidence

LLM-based augmentative retrieval systems rely on two complementary paradigms:

- **Sparse Retrieval (BM25/keyword search):** High precision on literal matches, IDs, domain-specific terms. Unmatched for “needle-in-the-haystack” queries.
- **Dense Retrieval (Embeddings):** Captures meaning, synonyms ("car" ~ "automobile"), paraphrasing, conceptual similarity.

**Problem:** Neither is sufficient alone—BM25 misses meaning, embeddings miss rare tokens and exactness. In hybrid, we seek the best of both worlds.

Anthropic’s evaluation confirms:

- "Contextual Embeddings" reduce retrieval failures by 35%.
- Adding Contextual BM25 brings this to 49%.
- Adding a learned reranking model reduces it further, to 67%.

### 2. Fusion Techniques: Ways to Combine Scores

Hybrid retrieval involves three steps:

1. Run BM25 and embedding search independently.
2. Normalize and fuse the scores or ranks.
3. Deduplicate, rerank, and present top-K results.

**Fusing the lists:** Multiple strategies exist:

- **Weighted Score Sum:** `final = α × embedding_score + (1-α) × bm25_score`
- **Reciprocal Rank Fusion (RRF):** `score = Σ[1/(k + rankᵢ)]` over sources (robust to score scale; no tuning required)
- **Rerankers:** Cross-encoders or LLMs rescore the merged set.

For example, if BM25 brings in technical IDs and embeddings return conceptually related chunks, RRF ensures both are surfaced—vital for LLM RAG systems.

### 3. Impact in Practice: Hybrid Retrieval in RAG, Search, and Reranking

**In Retrieval-Augmented Generation**:

- BM25 ensures the LLM is grounded in facts, codes, technical docs.
- Embeddings prevent “semantic gaps” (different wording, same concept).
- Hybrid boosts recall, reduces hallucinations and missed matches.

**Recent research and benchmarks (e.g. BEIR) consistently show** hybrid pipelines outperform either retriever alone—in recall, nDCG, and user-centric relevance. In real-world RAG deployments at Anthropic, OpenAI, and Databricks, hybrid is now standard practice.

### 4. BM25 in Modern LLM/RAG Pipelines

Some practical, illustrative examples:

- **Elasticsearch with vector plugins:** Use BM25 for first pass, rerank with vector semantic similarity (e.g. OpenSearch, Weaviate, Qdrant).
- **In-memory “rank_bm25 + FAISS” in Python:** Fetch BM25 top-100, then rerank using embedding similarity.
- **Weaviate’s hybrid mode:** Fuses BM25 and embedding search via score weighting, supports field-aware BM25 (BM25F).
- **Postgres (pgvector + FTS):** Uses BM25 for filters, then embedding similarity for semantic rerank—optimized for cost and latency.

## III. BM25 and Hybrid Search Implementations: Comparison and Analysis

### 1. Overview Table: BM25 Implementations

| Implementation                 | Type        | Language/API | BM25 Support | Hybrid Support | Notes                                      |
| ------------------------------ | ----------- | ------------ | ------------ | -------------- | ------------------------------------------ |
| Apache Lucene/Elasticsearch    | Self-hosted | Java/Python  | Yes          | Via plugin     | Best-in-class scaling, BM25 default        |
| BM25S                          | Self-hosted | Python       | Yes          | No             | Ultrafast, sparse-matrix, HuggingFace Hub  |
| rank_bm25                      | Self-hosted | Python       | Yes          | No             | Simple, RAM based, 100k doc limit          |
| Whoosh                         | Self-hosted | Python       | Yes (BM25F)  | No             | Fielded BM25, persistent, slower writes    |
| Qdrant                         | SaaS/Self   | API/Go/Py    | Yes (hybrid) | Yes            | Vector DB with hybrid support              |
| Pinecone                       | SaaS        | API          | Yes (hybrid) | Yes            | Hybrid, BM25 scoring, easy scaling         |
| Weaviate                       | SaaS/Self   | API          | Yes          | Yes            | Full hybrid stack, RESTful/GraphQL         |
| FAISS                          | Self-hosted | Py/C++       | No           | Vector Only    | Add BM25 via external index                |
| BM25-Golang (lenaxia, crawlab) | Self-hosted | Go           | Yes          | No/Partial     | Port of rank_bm25, stable and fast         |
| go-bm25                        | Self-hosted | Go/Py        | Yes          | No             | Fast, optimized, Python bindings, Postgres |
| Comet                          | Self-hosted | Go           | Yes          | Yes            | RRF hybrid, dual (BM25+vector), demo-ready |

**References**:

### 2. Discussion of Strengths, Limitations, and Use Cases

### a. Self-hosted Open Source Libraries

- **BM25S (Python):** Fastest for in-memory (<1M docs), sparse matrices, plug-and-play with HuggingFace. Not for multi-node scale.
- **rank_bm25:** Easiest for proof-of-concept, small-scale. Lacks persistence, not suited for >100K docs.
- **Go Libraries (lenaxia/crawlab, go-bm25, Comet):** Designed for performance and concurrency; Go's memory model aids scalable, parallel chunk ranking. Several (Comet, pentney/go-bm25) now include batch mode, RRF fusion, Postgres and vector DB integration.

### b. SaaS & API Providers

- **Qdrant, Pinecone, Weaviate:** All offer hybrid search, REST/gRPC APIs, and tight integration with LLM RAG stacks (LangChain, LlamaIndex). Support on-demand scaling, durable indexed storage, built-in chunking, native BM25/embedding fusion, and reranking utilities. Especially relevant for production hybrid deployment.

### c. Search Engines

- **Elasticsearch/Lucene:** Battle-tested, cloud-managed, best for large collections. Full field weighting (BM25F), analyzer flexibility, and easy hybrid via plugin or script.
- **Whoosh:** Python-focused, pure BM25/BM25F, single-node, lower speed.

### d. Limitations

- **BM25 (all):** No semantic awareness, no embedding. Hybridization required for semantic search.
- **Dense (FAISS, embeddings):** Weak on factual IDs, unique terms, and non-English nuances unless fine-tuned.

## IV. Best Practices: Chunking, Embedding, Indexing, and Contextualization

### 1. Chunking: Principles and Strategies

**Purpose:** To produce text units (“chunks”) that balance context and granularity, optimize for both BM25 and embedding retrieval, and fit LLM prompt windows.

**Chunking critically affects retrieval accuracy:**

- Too large: Loss of granularity, context window overloads.
- Too small: Loss of meaning, missed context, poor LLM grounding.

**Common chunking approaches**:

- **Fixed-size (tokens):** Easiest—e.g., 512–1000 tokens, with 10–20% overlap for context continuity.
- **Sentence/paragraph-based:** Retains semantic boundaries, ideal for prose and documentation.
- **Semantic/split-on-theme:** Advanced, group sentences by meaning via embeddings.
- **Recursive splitters:** LangChain, spaCy, NLTK, open-source splitters enable hybrid, language-aware chunking.

**Contextual Chunking (Anthropic’s breakthrough):**

- For each chunk, prepend a succinct LLM-generated summary situating the chunk within the whole document.

> E.g., Chunk: “... population is 3.8 million ...”
> Contextualized: “Berlin is the capital of Germany. Its population is 3.8 million ...”

Adds critical context for rare/ambiguous terms, grounding both embeddings and BM25 indices.

### 2. Embedding Generation and Storage

- **Generate dense vector embeddings** for each (contextualized) chunk with a consistent model (OpenAI, Cohere, Gemma, BGE, Mistral, etc.).
- **Choose embedding size** balanced with speed (128, 256, 512, 768, etc. for adaptive precision).
- **Store embeddings in a vector database** (Qdrant, Pinecone, Weaviate, FAISS) for fast nearest neighbor retrieval.
- **Persist or cache**; do not recompute at query time if possible.

**Embedding best practices**:

- Use token-based chunking, not just char-based.
- Use overlap to include boundary information.
- For RAG, store both sparse (BM25) and dense (embeddings) representations for each chunk.

### 3. Building and Updating BM25 Indices

- **Index corpus using BM25 on chunks, not full docs**.
- **Choose analyzers appropriate to language/task:** Customize stopwords, stemming, tokenization.
- **Persist indices** for large corpora (e.g., in Postgres FTS, Lucene, or dedicated IR stores).
- **Update/retrain BM25 encoders regularly** if adding many new documents/tokens; otherwise, use persistent serialization or batching updates for efficiency.

### 4. Contextual Retrieval: Synthesis

Anthropic’s methodology illustrates the power of embedding _contextualized_ chunks for both BM25 and vector search, using the same context-augmented chunk for both indices.

**End-to-end contextual retrieval pipeline:**

1. Preprocess and chunk documents (naive splitter);
2. Use a model (e.g., Claude, GPT-4o) to write a short description to prepend to each chunk;
3. Store both contextualized chunks and their embeddings;
4. At query time, retrieve top K using BM25 and vector search, fuse rankings, rerank, and pass results to LLM.

**Key finding:** Contextual chunking improves both BM25 and embedding recall—and their hybrid substantially reduces LLM "misses."

## V. Hybrid Search Architecture and Integration in Go

### 1. Go Ecosystem: BM25 and Embedding Libraries

**Sparse (BM25):**

- lenaxia/bm25-golang and crawlab-team/bm25: Classical BM25, BM25L, BM25+, BM25-Adpt, BM25T implementations, customizable, well-documented, stable for Go 1.25+.
- go-bm25: Optimized Go core, Python bindings, Postgres integration, batch/parallel processing, smart tokenization, and parameter tuning.
- wizenheimer/comet: All-in-one vector, BM25, and hybrid index with search and RRF fusion.

**Embeddings:**

- openai/openai-go: Official SDK for OpenAI’s embedding endpoints, Go-native.
- langchaingo (tmc/langchaingo): Go-native client for OpenAI and other embedding/LLM APIs, with practical embedding examples.
- Qdrant Go SDK: For vector store and API integration.

**Hybrid:**

- comet: Provides unified hybrid search over BM25 + vectors, RRF, batch scoring, and detailed Go API.
- Custom pipelines: BM25 results + external embedding API + fusion in Go.

### 2. Go: Minimal Example for Hybrid BM25 + Embeddings Search

The below code sketch leverages BM25 (crawlab/lenaxia, or go-bm25), OpenAI embedding API, and in-memory vector scoring. In practice, you’ll need to wire up serialization, vector storage, and normalization.

```go
package main

import (
    "fmt"
    "log"
    "strings"
    "sort"

    "github.com/crawlab-team/bm25"
    openai "github.com/openai/openai-go/v3"
    "github.com/openai/openai-go/v3/option"
)

// --- CONFIGURE YOUR API KEYS AND CHUNKS HERE ---
var apiKey = "sk-..." // Set OPENAI_API_KEY in env for safety

// Example corpus of (contextualized) chunks
var corpus = []string{
    "This chunk introduces RAG and contextual retrieval methodology.",
    "In BM25, relevance is based on term frequency and document length normalization.",
    "Embeddings enable semantic similarity for fuzzy search and paraphrase.",
    "Hybrid retrieval combines sparse (BM25) and dense (embedding) scoring for optimal recall.",
}

// Simple whitespace tokenizer
func tokenizer(s string) []string {
    return strings.Fields(strings.ToLower(s))
}

// Get dense embedding for a string from OpenAI
func getEmbedding(text string) ([]float32, error) {
    client := openai.NewClient(option.WithAPIKey(apiKey))
    resp, err := client.Embeddings.New(openai.EmbeddingsNewParams{
        Model: openai.EmbeddingModelTextEmbedding3Small,
        Input: []string{text},
    })
    if err != nil {
        return nil, err
    }
    vec := resp.Data[0].Embedding
    // Convert from []float64 to []float32
    out := make([]float32, len(vec))
    for i, v := range vec {
        out[i] = float32(v)
    }
    return out, nil
}

// Compute cosine similarity between two float32 vectors
func cosine(a, b []float32) float64 {
    var dot, an, bn float64
    for i := range a {
        dot += float64(a[i]) * float64(b[i])
        an += float64(a[i]) * float64(a[i])
        bn += float64(b[i]) * float64(b[i])
    }
    if an == 0 || bn == 0 {
        return 0
    }
    return dot / (sqrt(an) * sqrt(bn))
}
func sqrt(x float64) float64 {
    return x * 0.5 * (1 + x/(x*x + 1))
}

// Main hybrid retriever: computes normalized BM25 and cosine scores, fuses for top-k
func hybridSearch(query string, topK int, alpha float64) []struct{ idx int; combined float64 } {
    // (1) BM25 scores
    bm25Model, _ := bm25.NewBM25Okapi(corpus, tokenizer, nil)
    bm25Scores, _ := bm25Model.GetScores(tokenizer(query))
    maxBM25 := max(bm25Scores)
    minBM25 := min(bm25Scores)
    bm25Norm := normSlice(bm25Scores, minBM25, maxBM25)

    // (2) Embedding scores
    queryEmb, _ := getEmbedding(query)
    vecs := make([][]float32, len(corpus))
    for i, text := range corpus {
        vec, _ := getEmbedding(text)
        vecs[i] = vec
    }
    embedScores := make([]float64, len(corpus))
    for i, e := range vecs {
        embedScores[i] = cosine(queryEmb, e)
    }
    maxE := max(embedScores)
    minE := min(embedScores)
    embedNorm := normSlice(embedScores, minE, maxE)

    // (3) Fuse
    type result struct{ idx int; combined float64 }
    results := make([]result, len(corpus))
    for i := range corpus {
        // Linear weighted sum; alpha: weighting for embedding, (1-alpha) for BM25
        results[i] = result{
            idx:      i,
            combined: alpha*embedNorm[i] + (1-alpha)*bm25Norm[i],
        }
    }
    // Sort and pick top-K
    sort.Slice(results, func(i, j int) bool {
        return results[i].combined > results[j].combined
    })
    return results[:topK]
}

func main() {
    query := "neural retrieval with keyword and semantic search"
    alpha := 0.6 // higher=weight to semantic
    topK := 2
    results := hybridSearch(query, topK, alpha)
    for _, res := range results {
        fmt.Printf("Score: %.3f | Chunk: %s\n", res.combined, corpus[res.idx])
    }
}

// --- helper functions ---
func max(ss []float64) float64 {
    m := ss[0]
    for _, s := range ss { if s > m { m = s } }
    return m
}
func min(ss []float64) float64 {
    m := ss[0]
    for _, s := range ss { if s < m { m = s } }
    return m
}
func normSlice(vals []float64, minV float64, maxV float64) []float64 {
    out := make([]float64, len(vals))
    denom := maxV - minV
    if denom == 0 { denom = 1 }
    for i, v := range vals {
        out[i] = (v - minV) / denom
    }
    return out
}
```

**Key Points in the Example:**

- Text is first tokenized for BM25 and embedded for dense similarity.
- Both BM25 and embedding similarity scores are normalized to [0,1].
- Fusion uses a linear weighting parameter; RRF or other strategies (see e.g., Comet or Haystack) can be plugged in for more robust ranking.
- This is easily extensible to larger corpora, batch embedding, and DB/vector store persistence.

**Real-world Go production deployments would further**:

- Channel/coroutine BM25 and embedding scoring for speed.
- Cache or serialize embeddings and BM25 indices.
- Implement RRF or a reranker for result fusion and deduplication.
- Integrate with scalable stores like Postgres FTS and Qdrant vector DBs.

## VI. Performance, Evaluation Metrics, and Operational Trade-offs

### 1. Evaluating Hybrid Search

Typical metrics for retrieval effectiveness:

- **Recall@k:** Fraction of queries with at least one relevant doc in top-k.
- **Precision@k:** Fraction of docs in top-k that are relevant.
- **nDCG (normalized Discounted Cumulative Gain):** Penalizes lower-ranked correct hits, rewards top-k precision.
- **MRR (mean reciprocal rank):** Focus on how quickly a relevant doc appears.

**Empirical studies show**:

- BM25 is frequently superior to embeddings on out-of-domain data, technical and factual queries.
- Embeddings best for fuzzy/conceptual/semantic queries.
- Hybrid wins overall: higher recall, better nDCG, lower failure rate, more robust across domains.

### 2. Performance Considerations

- **BM25:** Fast on CPU, index fits in RAM/disk, optimal for sparse retrieval.
- **Embeddings:** One-time compute/storage cost per chunk, “approximate nearest neighbor” search for scaling (e.g., FAISS, Qdrant).
- **Hybrid overhead:** Two retrieval passes per query. For latency-sensitive applications, batch queries and vector cache.

**Scaling tips**:

- For streaming ingestion, periodically re-index BM25.
- For large data, persist embeddings and BM25 index.
- Use async Go routines for retrieval fusion on server.

### 3. Fusion Parameter and Query-Side Tuning

- Tune α in hybrid fusion: 0.3=more BM25; 0.7=more embeddings. Adjust per query type for optimal domain-specific results.
- RRF (Reciprocal Rank Fusion) is highly robust to scale differences and safest out-of-the-box.

## VII. Conclusion

BM25, a decades-old but ever-relevant algorithm, remains foundational for irreproducible, fast, and interpretable retrieval in modern AI workflows. In the context of LLMs and contextualized pipelines such as Anthropic’s RAG, combining BM25 with state-of-the-art embeddings is no longer optional, but a necessity. Hybrid retrieval fuses keyword exactness and semantic breadth, routinely outperforming either approach alone, especially when paired with chunk context augmentation and reranking.

**For Go practitioners and backend engineers:**

- Leverage stable Go BM25 and embedding libraries, integrate OpenAI or local embedding APIs, and use simple fusion approaches for best-in-class, production-grade hybrid search.
- Adopt best practices in chunking, contextualization, and score normalization.
- Use cloud/prod-ready hybrid vector stores (Qdrant, Pinecone) if you need scalability, but local indices and open-source stacks are now strong alternatives for many use cases.

**To future-proof your LLM retrieval:**
Embrace hybrid, contextualized RAG with BM25 and embeddings—your users, your models, and your metrics will thank you.
